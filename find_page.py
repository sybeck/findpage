import time
import re
from urllib.parse import urlparse, urlunparse, parse_qs

import requests

# ----------------------------
# Settings
# ----------------------------
NOT_FOUND_KEYWORDS = [
    "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì¡´ì¬í•˜ì§€",
    "ì‚­ì œëœ", "íŒë§¤ì¤‘ì§€", "ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
    "ì—†ëŠ” ìƒí’ˆ", "not found", "404"
]

SLEEP_SEC = 1.0
STOP_AFTER_CONSECUTIVE_MISSES = 100
TIMEOUT_SEC = 10

# âœ… ë¹„ì •ìƒ ìƒí™© ê°ì§€: ì—°ì†ìœ¼ë¡œ "FOUND"ê°€ ë„ˆë¬´ ì˜¤ë˜ ì§€ì†ë˜ëŠ” ê²½ìš°
STOP_AFTER_CONSECUTIVE_HITS = 200

USER_AGENT = "Mozilla/5.0 (compatible; ProductPageScanner/2.2)"

# ----------------------------
# URL utils
# ----------------------------
def ensure_scheme(url: str) -> str:
    url = url.strip()
    if not re.match(r"^https?://", url, re.IGNORECASE):
        return "https://" + url
    return url

def normalize_home(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, "", "", "", ""))

def strip_query_fragment(url: str) -> str:
    """
    Remove ?query and #fragment for stable path detection.
    """
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, p.path, "", "", ""))

def is_homepage(url: str) -> bool:
    """
    âœ… 'ì—†ëŠ” ìƒí’ˆ â†’ í™ˆ/ì¸ë±ìŠ¤ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸'ë¥¼ ì¡ê¸° ìœ„í•´ í™ˆ íŒë³„ì„ ë„“ê²Œ.
    """
    p = urlparse(ensure_scheme(url))
    path = (p.path or "").lower().strip()

    # "/" ë˜ëŠ” "" (ê¸°ë³¸ í™ˆ)
    if path in ["", "/"]:
        return True

    # í”í•œ í™ˆ/ì¸ë±ìŠ¤ ê²½ë¡œ
    home_like_paths = {
        "/index.html",
        "/index.htm",
        "/index.php",
        "/index.asp",
        "/index.aspx",
        "/default.asp",
        "/default.aspx",
        "/main",
        "/main/",
        "/main/index.html",
        "/main/index.htm",
        "/main/index.php",
    }
    if path in home_like_paths:
        return True

    return False

def normalize_for_compare(url: str) -> str:
    """
    URL ë¹„êµìš© ì •ê·œí™” (ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±° + í˜¸ìŠ¤íŠ¸/ìŠ¤í‚´ ì†Œë¬¸ì + trailing slash ì œê±°)
    """
    p = urlparse(ensure_scheme(url))
    path = (p.path or "").rstrip("/")
    return f"{p.scheme.lower()}://{p.netloc.lower()}{path}"

# ----------------------------
# Product ID extraction
# ----------------------------
def extract_product_id_from_input_url(product_url: str) -> int | None:
    """
    Extract product id from supported input URL patterns.
    - Cafe24 A: /surl/p/{id}
    - Cafe24 B: /product/.../{id}/category/...  (id is right before '/category/')
    - Cafe24 C: /product/detail.html?product_no={id}
    - Imweb:    /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    p_clean = urlparse(clean)
    p_raw = urlparse(raw)

    path = p_clean.path or ""
    query = p_raw.query or ""

    # Cafe24 A
    m = re.search(r"/surl/p/(\d+)", path)
    if m:
        return int(m.group(1))

    # Cafe24 B (id right before /category/)
    m = re.search(r"/product/.+/(\d+)/category/", path)
    if m:
        return int(m.group(1))

    # Cafe24 C: /product/detail.html?product_no=819
    if path.rstrip("/").lower().endswith("/product/detail.html"):
        qs = parse_qs(query)
        if "product_no" in qs and qs["product_no"]:
            v = qs["product_no"][0]
            if v.isdigit():
                return int(v)

    # Imweb idx
    if path.rstrip("/").lower().endswith("/product"):
        qs = parse_qs(query)
        if "idx" in qs and qs["idx"]:
            v = qs["idx"][0]
            if v.isdigit():
                return int(v)

    return None

# ----------------------------
# Platform detection (+ scan template policy)
# ----------------------------
def detect_platform_from_product_url(product_url: str):
    """
    Supported patterns:
    - Cafe24:
        * /surl/p/{id}
        * /product/.../{id}/category/...
        * /product/detail.html?product_no={id}
      âœ… Policy: If detected as Cafe24, scanning MUST ALWAYS use /surl/p/{id}

    - Imweb:
        * /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    parsed_clean = urlparse(clean)
    parsed_raw = urlparse(raw)

    path = parsed_clean.path or ""
    query = parsed_raw.query or ""
    base = normalize_home(clean)

    # -----------------
    # Cafe24 (ALL CASES) -> always scan with /surl/p/{id}
    # -----------------
    if "/surl/p/" in path and re.search(r"/surl/p/\d+", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    if path.startswith("/product/") and re.search(r"/product/.+/\d+/category/", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    if path.rstrip("/").lower().endswith("/product/detail.html"):
        if re.search(r"(?:^|&)product_no=\d+(?:&|$)", query, re.IGNORECASE):
            return "cafe24", f"{base}/surl/p/{{id}}"

    # -----------------
    # Imweb
    # -----------------
    if path.rstrip("/").lower().endswith("/product"):
        if re.search(r"(?:^|&)idx=\d+(?:&|$)", query, re.IGNORECASE):
            return "imweb", f"{base}/Product/?idx={{id}}"

    return None, None

# ----------------------------
# Product name parsing
# ----------------------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def extract_product_name(html: str) -> str:
    patterns = [
        #r'<meta[^>]+property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']',
        #r'<meta[^>]+name=["\']twitter:title["\'][^>]*content=["\']([^"\']+)["\']',
        r"<title[^>]*>(.*?)</title>",
    ]
    for pat in patterns:
        m = re.search(pat, html, re.IGNORECASE | re.DOTALL)
        if m:
            return clean_text(m.group(1))
    return "(ì œí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨)"

# ----------------------------
# Not-found íŒë‹¨ (âœ… ì›ë˜ ì•„ì´ë””ì–´ëŒ€ë¡œ: í™ˆ/ì¸ë±ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ëŠ” NOT FOUND)
# ----------------------------
def looks_not_found(status_code: int, requested_url: str, final_url: str, html: str) -> bool:
    if status_code != 200:
        return True

    # âœ… ì—†ëŠ” ìƒí’ˆì´ë©´ í™ˆ/ì¸ë±ìŠ¤ ê³„ì—´ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ì¼€ì´ìŠ¤
    req = normalize_for_compare(requested_url)
    fin = normalize_for_compare(final_url)
    if req != fin and is_homepage(final_url):
        return True

    sample = (html[:20000] or "").lower()
    for kw in NOT_FOUND_KEYWORDS:
        if kw in sample:
            return True

    if len(sample.strip()) < 200:
        return True

    return False

# ----------------------------
# Scanner (1-pass)
# ----------------------------
def scan_pass(
    template_url: str,
    start_id: int,
    stop_after_consecutive_misses: int,
    sleep_sec: float,
    allow_extra_retry_if_zero_found: bool,
    found_products: list[tuple[str, str]] | None = None,
    found_urls: set[str] | None = None,
):
    """
    One scanning pass.
    - starts from start_id
    - stops when consecutive misses reach stop_after_consecutive_misses
    - optional extra retry ONLY when allow_extra_retry_if_zero_found=True
      and found_products is still empty at first stop trigger.

    âœ… ì¶”ê°€ ë³´í˜¸:
    - ì—°ì† STOP_AFTER_CONSECUTIVE_HITS(ê¸°ë³¸ 200)ë²ˆ FOUNDê°€ ë‚˜ì˜¤ë©´ ë¹„ì •ìƒìœ¼ë¡œ ë³´ê³  ì—ëŸ¬ ë°œìƒ
      (ì˜ˆ: ëª¨ë“  ìš”ì²­ì´ ì–´ë–¤ ê³µí†µ í˜ì´ì§€ë¡œ "FOUND"ë¡œ íŒì •ë˜ëŠ” ê²½ìš°)
    """
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    product_id = start_id
    consecutive_misses = 0
    consecutive_hits = 0
    extra_retry_used = False

    if found_products is None:
        found_products = []
    if found_urls is None:
        found_urls = set()

    while True:
        url = template_url.format(id=product_id)
        print(f"[CHECK] {url}")

        try:
            r = session.get(url, allow_redirects=True, timeout=TIMEOUT_SEC)

            if looks_not_found(r.status_code, url, r.url, r.text or ""):
                consecutive_misses += 1
                consecutive_hits = 0
                print(f"  -> NOT FOUND ({consecutive_misses}/{stop_after_consecutive_misses})")
            else:
                consecutive_misses = 0
                consecutive_hits += 1

                final_url = r.url
                p_name = ""
                if final_url not in found_urls:
                    name = extract_product_name(r.text or "")
                    found_products.append((name, final_url))
                    found_urls.add(final_url)
                    p_name = name

                print(f"  âœ… FOUND: {p_name}\n{final_url} ({consecutive_hits}/{STOP_AFTER_CONSECUTIVE_HITS})")

                # âœ… ë¹„ì •ìƒ ê°ì§€: ì—°ì†ìœ¼ë¡œ ë„ˆë¬´ ë§ì´ FOUND
                if consecutive_hits >= STOP_AFTER_CONSECUTIVE_HITS:
                    raise RuntimeError(
                        f"ë¹„ì •ìƒ ê°ì§€: ì—°ì† {STOP_AFTER_CONSECUTIVE_HITS}ê°œê°€ 'FOUND'ë¡œ íŒì •ë˜ì—ˆìŠµë‹ˆë‹¤. "
                        f"NOT FOUND íŒì •ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ëª¨ë“  ìš”ì²­ì´ ê³µí†µ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ìƒí™©ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                        f"(ì˜ˆ: ë§ˆì§€ë§‰ ìš”ì²­ URL: {url}, ìµœì¢… URL: {final_url})"
                    )

        except requests.RequestException as e:
            consecutive_misses += 1
            consecutive_hits = 0
            print(f"  -> ERROR: {e} ({consecutive_misses}/{stop_after_consecutive_misses})")

        if consecutive_misses >= stop_after_consecutive_misses:
            if allow_extra_retry_if_zero_found and (len(found_products) == 0) and (not extra_retry_used):
                print(f"\n[INFO] ì•„ì§ ì œí’ˆì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í•´ ì¶”ê°€ {stop_after_consecutive_misses}íšŒ ìŠ¤ìº”ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                consecutive_misses = 0
                extra_retry_used = True
            else:
                break

        product_id += 1
        time.sleep(sleep_sec)

    return found_products, found_urls

# ----------------------------
# Main
# ----------------------------
def main():
    print("ì œí’ˆ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (UTM í¬í•¨ ê°€ëŠ¥)")
    print("ì˜ˆ) https://brainology.kr/surl/p/10")
    print("ì˜ˆ) https://brainology.kr/product/.../10/category/24/display/1/  (ì¹´í˜24 ê°ì§€ìš©, ìŠ¤ìº”ì€ /surl/p/{id})")
    print("ì˜ˆ) https://drphytomall.com/product/detail.html?product_no=819  (ì¹´í˜24 ê°ì§€ìš©, ìŠ¤ìº”ì€ /surl/p/{id})")
    print("ì˜ˆ) https://www.realcumin.kr/Product/?idx=72")
    product_url = input("> ").strip()

    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        print("\n[ERROR] ì²˜ìŒ ë³´ëŠ” í˜ì´ì§€ íŒ¨í„´ì…ë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        print("\n[ERROR] ì…ë ¥ URLì—ì„œ ì œí’ˆ idë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    print(f"\n[INFO] í”Œë«í¼: {platform}")
    print(f"[INFO] ì‹¤ì œ ìŠ¤ìº” URL íŒ¨í„´: {template_url}")
    print(f"[INFO] ì…ë ¥ URL ì œí’ˆ id: {input_product_id}")
    print(f"[INFO] ì¤‘ë‹¨ ê¸°ì¤€: ì—°ì† {STOP_AFTER_CONSECUTIVE_MISSES}íšŒ NOT FOUND/ERROR")
    print(f"[INFO] ë¹„ì •ìƒ ê¸°ì¤€: ì—°ì† {STOP_AFTER_CONSECUTIVE_HITS}íšŒ FOUNDë©´ ì—ëŸ¬")
    print(f"[INFO] ìŠ¤ìº” ì†ë„: {SLEEP_SEC}ì´ˆì— 1íšŒ")
    print("\n[START] 1ì°¨ ìŠ¤ìº” (start=1)\n")

    # 1) First pass: start at 1, with "extra retry" if zero found
    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=1,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=set(),
    )

    # 2) Conditional second pass
    threshold = input_product_id * 0.01
    if len(found_products) < threshold:
        print("\n" + "-" * 60)
        print("[INFO] ì¶”ê°€ ì¡°ê±´ íŠ¸ë¦¬ê±°!")
        print(f"[INFO] 1ì°¨ ë°œê²¬ ê°œìˆ˜({len(found_products)}) < ì…ë ¥ ì œí’ˆ id * 0.01 ({threshold:.2f})")
        print(f"[INFO] 2ì°¨ ìŠ¤ìº”ì„ ì…ë ¥ ì œí’ˆ id({input_product_id})ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        print("-" * 60 + "\n")

        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,
            found_products=found_products,
            found_urls=found_urls,
        )
    else:
        print("\n[INFO] ì¶”ê°€ 2ì°¨ ìŠ¤ìº” ì¡°ê±´ ë¯¸ì¶©ì¡± (ì¶”ê°€ ìŠ¤ìº” ì—†ìŒ)")

    # Final summary
    print("\n" + "=" * 50)
    print("ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ (ì œí’ˆëª… + URL)")
    print("=" * 50)

    if not found_products:
        print("ì°¾ì€ ì œí’ˆ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for idx, (name, url) in enumerate(found_products, 1):
        print(f"{idx}. {name}")
        print(f"   {url}")

    print("\nì´ ë°œê²¬ ì œí’ˆ ìˆ˜:", len(found_products))

def scan_for_slack(product_url: str):
    """
    Slack botìš© ì—”íŠ¸ë¦¬ í•¨ìˆ˜
    """
    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        raise ValueError("Unsupported product URL pattern")

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        raise ValueError("Failed to extract product id from URL")

    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=1,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=set(),
    )

    if len(found_products) < (input_product_id * 0.01):
        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,
            found_products=found_products,
            found_urls=found_urls,
        )

    return found_products

if __name__ == "__main__":
    main()
