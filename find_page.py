import time
import re
from urllib.parse import urlparse, urlunparse

import requests

# ----------------------------
# Settings
# ----------------------------
NOT_FOUND_KEYWORDS = [
    "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì¡´ì¬í•˜ì§€",
    "ì‚­ì œëœ", "íŒë§¤ì¤‘ì§€", "ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
    "ì—†ëŠ” ìƒí’ˆ", "not found", "404"
]

SLEEP_SEC = 1.0
STOP_AFTER_CONSECUTIVE_MISSES = 100
TIMEOUT_SEC = 10

USER_AGENT = "Mozilla/5.0 (compatible; ProductPageScanner/1.7)"

# ----------------------------
# URL utils
# ----------------------------
def ensure_scheme(url: str) -> str:
    url = url.strip()
    if not re.match(r"^https?://", url, re.IGNORECASE):
        return "https://" + url
    return url

def normalize_home(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, "", "", "", ""))

def strip_query_fragment(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, p.path, "", "", ""))

def is_homepage(url: str) -> bool:
    p = urlparse(url)
    return (p.path or "").rstrip("/") in ["", "/"]

# ----------------------------
# Platform detection (ê°ì§€ìš© íŒ¨í„´ í™•ì¥)
# ----------------------------
def detect_platform_from_product_url(product_url: str):
    """
    ê°ì§€ìš© íŒ¨í„´:
    - Cafe24:
        1) /surl/p/{id}
        2) /product/.../{id}/category/...
       â†’ ê°ì§€ í›„ ìŠ¤ìº”ì€ í•­ìƒ /surl/p/{id}
    - Imweb:
        /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    parsed_clean = urlparse(clean)
    parsed_raw = urlparse(raw)

    path = parsed_clean.path or ""
    query = parsed_raw.query or ""
    base = normalize_home(clean)

    # ---- Cafe24 (A): /surl/p/{id}
    if "/surl/p/" in path and re.search(r"/surl/p/\d+", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    # ---- Cafe24 (B): /product/.../{id}/category/...
    if path.startswith("/product/") and re.search(r"/product/.+/\d+/category/", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    # ---- Imweb: /Product/?idx={id}
    if path.rstrip("/").lower().endswith("/product"):
        if re.search(r"(?:^|&)idx=\d+(?:&|$)", query, re.IGNORECASE):
            return "imweb", f"{base}/Product/?idx={{id}}"

    return None, None

# ----------------------------
# Product name parsing
# ----------------------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def extract_product_name(html: str) -> str:
    patterns = [
        r'<meta[^>]+property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']',
        r'<meta[^>]+name=["\']twitter:title["\'][^>]*content=["\']([^"\']+)["\']',
        r"<title[^>]*>(.*?)</title>",
    ]
    for pat in patterns:
        m = re.search(pat, html, re.IGNORECASE | re.DOTALL)
        if m:
            return clean_text(m.group(1))
    return "(ì œí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨)"

# ----------------------------
# Not-found íŒë‹¨
# ----------------------------
def looks_not_found(status_code: int, requested_url: str, final_url: str, html: str) -> bool:
    if status_code != 200:
        return True

    if requested_url.rstrip("/") != final_url.rstrip("/") and is_homepage(final_url):
        return True

    sample = (html[:20000] or "").lower()
    for kw in NOT_FOUND_KEYWORDS:
        if kw in sample:
            return True

    if len(sample.strip()) < 200:
        return True

    return False

# ----------------------------
# Scanner
# ----------------------------
def scan(template_url: str):
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    product_id = 1
    consecutive_misses = 0
    extra_retry_used = False

    found_products = []
    found_urls = set()

    while True:
        url = template_url.format(id=product_id)
        print(f"[CHECK] {url}")

        try:
            r = session.get(url, allow_redirects=True, timeout=TIMEOUT_SEC)

            if looks_not_found(r.status_code, url, r.url, r.text or ""):
                consecutive_misses += 1
                print(f"  -> NOT FOUND ({consecutive_misses})")
            else:
                consecutive_misses = 0
                final_url = r.url

                if final_url not in found_urls:
                    name = extract_product_name(r.text or "")
                    found_products.append((name, final_url))
                    found_urls.add(final_url)

                print(f"  âœ… FOUND: {final_url}")

        except requests.RequestException as e:
            consecutive_misses += 1
            print(f"  -> ERROR: {e} ({consecutive_misses})")

        # ì¢…ë£Œ ì¡°ê±´ (ì´ˆë°˜ 30ë²ˆ ì „ë¶€ ì‹¤íŒ¨ ì‹œ 1íšŒ ì¶”ê°€ í—ˆìš©)
        if consecutive_misses >= STOP_AFTER_CONSECUTIVE_MISSES:
            if not found_products and not extra_retry_used:
                print("\n[INFO] ì•„ì§ ì œí’ˆì„ ì°¾ì§€ ëª»í•´ ì¶”ê°€ 30íšŒ ìŠ¤ìº”ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                consecutive_misses = 0
                extra_retry_used = True
            else:
                break

        product_id += 1
        time.sleep(SLEEP_SEC)

    return found_products

# ----------------------------
# Main
# ----------------------------
def main():
    print("ì œí’ˆ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (UTM í¬í•¨ ê°€ëŠ¥)")
    print("ì˜ˆ) https://brainology.kr/surl/p/10")
    print("ì˜ˆ) https://brainology.kr/product/.../10/category/24/display/1/")
    print("ì˜ˆ) https://www.realcumin.kr/Product/?idx=72")
    product_url = input("> ").strip()

    platform, template_url = detect_platform_from_product_url(product_url)

    if not platform:
        print("\n[ERROR] ì²˜ìŒ ë³´ëŠ” í˜ì´ì§€ íŒ¨í„´ì…ë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    print(f"\n[INFO] í”Œë«í¼: {platform}")
    print(f"[INFO] ì‹¤ì œ ìŠ¤ìº” URL íŒ¨í„´: {template_url}")
    print("\n[START]\n")

    results = scan(template_url)

    print("\n" + "=" * 50)
    print("ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ (ì œí’ˆëª… + URL)")
    print("=" * 50)

    if not results:
        print("ì°¾ì€ ì œí’ˆ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for idx, (name, url) in enumerate(results, 1):
        print(f"{idx}. {name}")
        print(f"   {url}")

    print("\nì´ ë°œê²¬ ì œí’ˆ ìˆ˜:", len(results))


if __name__ == "__main__":
    main()
