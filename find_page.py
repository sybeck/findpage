import time
import re
import os
from urllib.parse import urlparse, urlunparse, parse_qs

import requests

# ----------------------------
# Settings
# ----------------------------
NOT_FOUND_KEYWORDS = [
    "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì¡´ì¬í•˜ì§€",
    "ì‚­ì œëœ", "íŒë§¤ì¤‘ì§€", "ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
    "ì—†ëŠ” ìƒí’ˆ", "not found", "404"
]

SLEEP_SEC = 1.0
STOP_AFTER_CONSECUTIVE_MISSES = 100
TIMEOUT_SEC = 10

# âœ… ë¹„ì •ìƒ ìƒí™© ê°ì§€: ì—°ì†ìœ¼ë¡œ "FOUND"ê°€ ë„ˆë¬´ ì˜¤ë˜ ì§€ì†ë˜ëŠ” ê²½ìš°
STOP_AFTER_CONSECUTIVE_HITS = 200

USER_AGENT = "Mozilla/5.0 (compatible; ProductPageScanner/2.2)"

# ----------------------------
# URL utils
# ----------------------------
def ensure_scheme(url: str) -> str:
    url = url.strip()
    if not re.match(r"^https?://", url, re.IGNORECASE):
        return "https://" + url
    return url

def normalize_home(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, "", "", "", ""))

def get_domain_from_url(url: str) -> str:
    """
    URLì—ì„œ ë„ë©”ì¸ëª…ì„ ì¶”ì¶œ (ì˜ˆ: 'brainology.kr')
    """
    u = ensure_scheme(url)
    p = urlparse(u)
    return p.netloc.replace('www.', '')

def strip_query_fragment(url: str) -> str:
    """
    Remove ?query and #fragment for stable path detection.
    """
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, p.path, "", "", ""))

def is_homepage(url: str) -> bool:
    """
    âœ… 'ì—†ëŠ” ìƒí’ˆ â†’ í™ˆ/ì¸ë±ìŠ¤ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸'ë¥¼ ì¡ê¸° ìœ„í•´ í™ˆ íŒë³„ì„ ë„“ê²Œ.
    """
    p = urlparse(ensure_scheme(url))
    path = (p.path or "").lower().strip()

    # "/" ë˜ëŠ” "" (ê¸°ë³¸ í™ˆ)
    if path in ["", "/"]:
        return True

    # í”í•œ í™ˆ/ì¸ë±ìŠ¤ ê²½ë¡œ
    home_like_paths = {
        "/index.html",
        "/index.htm",
        "/index.php",
        "/index.asp",
        "/index.aspx",
        "/default.asp",
        "/default.aspx",
        "/main",
        "/main/",
        "/main/index.html",
        "/main/index.htm",
        "/main/index.php",
    }
    if path in home_like_paths:
        return True

    return False

def normalize_for_compare(url: str) -> str:
    """
    URL ë¹„êµìš© ì •ê·œí™” (ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±° + í˜¸ìŠ¤íŠ¸/ìŠ¤í‚´ ì†Œë¬¸ì + trailing slash ì œê±°)
    """
    p = urlparse(ensure_scheme(url))
    path = (p.path or "").rstrip("/")
    return f"{p.scheme.lower()}://{p.netloc.lower()}{path}"

# ----------------------------
# Product ID extraction
# ----------------------------
def extract_product_id_from_input_url(product_url: str) -> int | None:
    """
    Extract product id from supported input URL patterns.
    - Cafe24 A: /surl/p/{id}
    - Cafe24 B: /product/.../{id}/category/...  (id is right before '/category/')
    - Cafe24 C: /product/detail.html?product_no={id}
    - Imweb:    /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    p_clean = urlparse(clean)
    p_raw = urlparse(raw)

    path = p_clean.path or ""
    query = p_raw.query or ""

    # Cafe24 A
    m = re.search(r"/surl/p/(\d+)", path)
    if m:
        return int(m.group(1))

    # Cafe24 B (id right before /category/)
    m = re.search(r"/product/.+/(\d+)/category/", path)
    if m:
        return int(m.group(1))

    # Cafe24 C: /product/detail.html?product_no=819
    if path.rstrip("/").lower().endswith("/product/detail.html"):
        qs = parse_qs(query)
        if "product_no" in qs and qs["product_no"]:
            v = qs["product_no"][0]
            if v.isdigit():
                return int(v)

    # Imweb idx
    if path.rstrip("/").lower().endswith("/product"):
        qs = parse_qs(query)
        if "idx" in qs and qs["idx"]:
            v = qs["idx"][0]
            if v.isdigit():
                return int(v)

    return None

# ----------------------------
# Platform detection (+ scan template policy)
# ----------------------------
def detect_platform_from_product_url(product_url: str):
    """
    Supported patterns:
    - Cafe24:
        * /surl/p/{id}
        * /product/.../{id}/category/...
        * /product/detail.html?product_no={id}
      âœ… Policy: If detected as Cafe24, scanning MUST ALWAYS use /surl/p/{id}

    - Imweb:
        * /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    parsed_clean = urlparse(clean)
    parsed_raw = urlparse(raw)

    path = parsed_clean.path or ""
    query = parsed_raw.query or ""
    base = normalize_home(clean)

    # -----------------
    # Cafe24 (ALL CASES) -> always scan with /surl/p/{id}
    # -----------------
    if "/surl/p/" in path and re.search(r"/surl/p/\d+", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    if path.startswith("/product/") and re.search(r"/product/.+/\d+/category/", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    if path.rstrip("/").lower().endswith("/product/detail.html"):
        if re.search(r"(?:^|&)product_no=\d+(?:&|$)", query, re.IGNORECASE):
            return "cafe24", f"{base}/surl/p/{{id}}"

    # -----------------
    # Imweb
    # -----------------
    if path.rstrip("/").lower().endswith("/product"):
        if re.search(r"(?:^|&)idx=\d+(?:&|$)", query, re.IGNORECASE):
            return "imweb", f"{base}/Product/?idx={{id}}"

    return None, None

# ----------------------------
# Product name parsing
# ----------------------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def extract_product_name(html: str) -> str:
    patterns = [
        #r'<meta[^>]+property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']',
        #r'<meta[^>]+name=["\']twitter:title["\'][^>]*content=["\']([^"\']+)["\']',
        r"<title[^>]*>(.*?)</title>",
    ]
    for pat in patterns:
        m = re.search(pat, html, re.IGNORECASE | re.DOTALL)
        if m:
            return clean_text(m.group(1))
    return "(ì œí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨)"

def extract_influencer_names(product_names: list[str]) -> list[str]:
    """
    ì œí’ˆëª… ë¦¬ìŠ¤íŠ¸ì—ì„œ "ë„¤", "ë§˜", "ì•½"ì´ í¬í•¨ëœ ë‹¨ì–´ë¥¼ ì¶”ì¶œ.
    ì˜ˆ: "ì„œìš¸ë„¤ì•½êµ­ í”„ë¦¬ë¯¸ì—„" -> ["ì„œìš¸ë„¤ì•½êµ­"]
    """
    influencers = set()
    target_chars = ['ë„¤', 'ë§˜', 'ì•½']
    
    for name in product_names:
        # ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¡œ ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', name)
        for word in words:
            # ë‹¨ì–´ì— "ë„¤", "ë§˜", "ì•½" ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì¶”ê°€
            if any(char in word for char in target_chars):
                influencers.add(word)
    
    return sorted(list(influencers))

# ----------------------------
# Not-found íŒë‹¨ (âœ… ì›ë˜ ì•„ì´ë””ì–´ëŒ€ë¡œ: í™ˆ/ì¸ë±ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ëŠ” NOT FOUND)
# ----------------------------
def looks_not_found(status_code: int, requested_url: str, final_url: str, html: str) -> bool:
    if status_code != 200:
        return True

    # âœ… ì—†ëŠ” ìƒí’ˆì´ë©´ í™ˆ/ì¸ë±ìŠ¤ ê³„ì—´ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ì¼€ì´ìŠ¤
    req = normalize_for_compare(requested_url)
    fin = normalize_for_compare(final_url)
    if req != fin and is_homepage(final_url):
        return True

    sample = (html[:20000] or "").lower()
    for kw in NOT_FOUND_KEYWORDS:
        if kw in sample:
            return True

    if len(sample.strip()) < 200:
        return True

    return False

# ----------------------------
# File I/O for domain-based tracking
# ----------------------------
def get_last_id_from_file(domain: str) -> int:
    """
    ë„ë©”ì¸ëª….txt íŒŒì¼ì—ì„œ ë§ˆì§€ë§‰ IDë¥¼ ì½ì–´ì˜´.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ ì½ê¸° ì‹¤íŒ¨ì‹œ 0ì„ ë°˜í™˜.
    """
    filename = f"{domain}.txt"
    if not os.path.exists(filename):
        return 0
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if not lines:
                return 0
            # ë§ˆì§€ë§‰ ë¼ì¸ì—ì„œ ID ì¶”ì¶œ (í˜•ì‹: "ë²ˆí˜¸. ì œí’ˆëª…\nURL")
            for line in reversed(lines):
                line = line.strip()
                if line.startswith('http'):
                    # URLì—ì„œ ID ì¶”ì¶œ
                    match = re.search(r'/(\d+)(?:[/?#]|$)', line)
                    if match:
                        return int(match.group(1))
            return 0
    except Exception as e:
        print(f"[WARNING] {domain}.txt íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return 0

def load_existing_products(domain: str) -> list[tuple[str, str]]:
    """
    ë„ë©”ì¸ëª….txt íŒŒì¼ì—ì„œ ê¸°ì¡´ ì œí’ˆ ëª©ë¡ì„ ë¡œë“œ.
    """
    filename = f"{domain}.txt"
    if not os.path.exists(filename):
        return []
    
    products = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                # "ë²ˆí˜¸. ì œí’ˆëª…" í˜•ì‹ ì°¾ê¸°
                if re.match(r'^\d+\.\s+', line):
                    name = re.sub(r'^\d+\.\s+', '', line)
                    # ë‹¤ìŒ ì¤„ì´ URLì¸ì§€ í™•ì¸
                    if i + 1 < len(lines):
                        url_line = lines[i + 1].strip()
                        if url_line.startswith('http'):
                            products.append((name, url_line))
                            i += 2
                            continue
                i += 1
    except Exception as e:
        print(f"[WARNING] {domain}.txt íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return products

def save_products_to_file(domain: str, products: list[tuple[str, str]]):
    """
    ì œí’ˆ ëª©ë¡ì„ ë„ë©”ì¸ëª….txt íŒŒì¼ë¡œ ì €ì¥.
    """
    filename = f"{domain}.txt"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for idx, (name, url) in enumerate(products, 1):
                f.write(f"{idx}. {name}\n")
                f.write(f"{url}\n\n")
        print(f"[INFO] {len(products)}ê°œ ì œí’ˆì„ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ERROR] {filename} ì €ì¥ ì‹¤íŒ¨: {e}")

def save_influencers_to_file(domain: str, influencers: list[str]) -> str:
    """
    ì¸í”Œë£¨ì–¸ì„œëª…ì„ ë„ë©”ì¸_influencers.txt íŒŒì¼ë¡œ ì €ì¥í•˜ê³  íŒŒì¼ëª… ë°˜í™˜.
    """
    filename = f"{domain}_influencers.txt"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("# ì¸í”Œë£¨ì–¸ì„œëª… ì¶”ì¶œ ê²°ê³¼\n")
            f.write("# 'ë„¤', 'ë§˜', 'ì•½'ì´ í¬í•¨ëœ ë‹¨ì–´ë“¤\n\n")
            for inf in influencers:
                f.write(f"{inf}\n")
        print(f"[INFO] {len(influencers)}ê°œ ì¸í”Œë£¨ì–¸ì„œëª…ì„ {filename}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        print(f"[ERROR] {filename} ì €ì¥ ì‹¤íŒ¨: {e}")
        return ""

# ----------------------------
# Scanner (1-pass)
# ----------------------------
def scan_pass(
    template_url: str,
    start_id: int,
    stop_after_consecutive_misses: int,
    sleep_sec: float,
    allow_extra_retry_if_zero_found: bool,
    found_products: list[tuple[str, str]] | None = None,
    found_urls: set[str] | None = None,
):
    """
    One scanning pass.
    - starts from start_id
    - stops when consecutive misses reach stop_after_consecutive_misses
    - optional extra retry ONLY when allow_extra_retry_if_zero_found=True
      and found_products is still empty at first stop trigger.

    âœ… ì¶”ê°€ ë³´í˜¸:
    - ì—°ì† STOP_AFTER_CONSECUTIVE_HITS(ê¸°ë³¸ 200)ë²ˆ FOUNDê°€ ë‚˜ì˜¤ë©´ ë¹„ì •ìƒìœ¼ë¡œ ë³´ê³  ì—ëŸ¬ ë°œìƒ
      (ì˜ˆ: ëª¨ë“  ìš”ì²­ì´ ì–´ë–¤ ê³µí†µ í˜ì´ì§€ë¡œ "FOUND"ë¡œ íŒì •ë˜ëŠ” ê²½ìš°)
    """
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    product_id = start_id
    consecutive_misses = 0
    consecutive_hits = 0
    extra_retry_used = False

    if found_products is None:
        found_products = []
    if found_urls is None:
        found_urls = set()

    while True:
        url = template_url.format(id=product_id)
        print(f"[CHECK] {url}")

        try:
            r = session.get(url, allow_redirects=True, timeout=TIMEOUT_SEC)

            if looks_not_found(r.status_code, url, r.url, r.text or ""):
                consecutive_misses += 1
                consecutive_hits = 0
                print(f"  -> NOT FOUND ({consecutive_misses}/{stop_after_consecutive_misses})")
            else:
                consecutive_misses = 0
                consecutive_hits += 1

                final_url = r.url
                p_name = ""
                if final_url not in found_urls:
                    name = extract_product_name(r.text or "")
                    found_products.append((name, final_url))
                    found_urls.add(final_url)
                    p_name = name

                print(f"  âœ… FOUND: {p_name}\n{final_url} ({consecutive_hits}/{STOP_AFTER_CONSECUTIVE_HITS})")

                # âœ… ë¹„ì •ìƒ ê°ì§€: ì—°ì†ìœ¼ë¡œ ë„ˆë¬´ ë§ì´ FOUND
                if consecutive_hits >= STOP_AFTER_CONSECUTIVE_HITS:
                    raise RuntimeError(
                        f"ë¹„ì •ìƒ ê°ì§€: ì—°ì† {STOP_AFTER_CONSECUTIVE_HITS}ê°œê°€ 'FOUND'ë¡œ íŒì •ë˜ì—ˆìŠµë‹ˆë‹¤. "
                        f"NOT FOUND íŒì •ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ëª¨ë“  ìš”ì²­ì´ ê³µí†µ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ìƒí™©ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                        f"(ì˜ˆ: ë§ˆì§€ë§‰ ìš”ì²­ URL: {url}, ìµœì¢… URL: {final_url})"
                    )

        except requests.RequestException as e:
            consecutive_misses += 1
            consecutive_hits = 0
            print(f"  -> ERROR: {e} ({consecutive_misses}/{stop_after_consecutive_misses})")

        if consecutive_misses >= stop_after_consecutive_misses:
            if allow_extra_retry_if_zero_found and (len(found_products) == 0) and (not extra_retry_used):
                print(f"\n[INFO] ì•„ì§ ì œí’ˆì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í•´ ì¶”ê°€ {stop_after_consecutive_misses}íšŒ ìŠ¤ìº”ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                consecutive_misses = 0
                extra_retry_used = True
            else:
                break

        product_id += 1
        time.sleep(sleep_sec)

    return found_products, found_urls

# ----------------------------
# Main
# ----------------------------
def main():
    print("ì œí’ˆ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (UTM í¬í•¨ ê°€ëŠ¥)")
    print("ì˜ˆ) https://brainology.kr/surl/p/10")
    print("ì˜ˆ) https://brainology.kr/product/.../10/category/24/display/1/  (ì¹´í˜24 ê°ì§€ìš©, ìŠ¤ìº”ì€ /surl/p/{id})")
    print("ì˜ˆ) https://drphytomall.com/product/detail.html?product_no=819  (ì¹´í˜24 ê°ì§€ìš©, ìŠ¤ìº”ì€ /surl/p/{id})")
    print("ì˜ˆ) https://www.realcumin.kr/Product/?idx=72")
    product_url = input("> ").strip()

    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        print("\n[ERROR] ì²˜ìŒ ë³´ëŠ” í˜ì´ì§€ íŒ¨í„´ì…ë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        print("\n[ERROR] ì…ë ¥ URLì—ì„œ ì œí’ˆ idë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    print(f"\n[INFO] í”Œë«í¼: {platform}")
    print(f"[INFO] ì‹¤ì œ ìŠ¤ìº” URL íŒ¨í„´: {template_url}")
    print(f"[INFO] ì…ë ¥ URL ì œí’ˆ id: {input_product_id}")
    print(f"[INFO] ì¤‘ë‹¨ ê¸°ì¤€: ì—°ì† {STOP_AFTER_CONSECUTIVE_MISSES}íšŒ NOT FOUND/ERROR")
    print(f"[INFO] ë¹„ì •ìƒ ê¸°ì¤€: ì—°ì† {STOP_AFTER_CONSECUTIVE_HITS}íšŒ FOUNDë©´ ì—ëŸ¬")
    print(f"[INFO] ìŠ¤ìº” ì†ë„: {SLEEP_SEC}ì´ˆì— 1íšŒ")
    print("\n[START] 1ì°¨ ìŠ¤ìº” (start=1)\n")

    # 1) First pass: start at 1, with "extra retry" if zero found
    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=1,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=set(),
    )

    # 2) Conditional second pass
    threshold = input_product_id * 0.01
    if len(found_products) < threshold:
        print("\n" + "-" * 60)
        print("[INFO] ì¶”ê°€ ì¡°ê±´ íŠ¸ë¦¬ê±°!")
        print(f"[INFO] 1ì°¨ ë°œê²¬ ê°œìˆ˜({len(found_products)}) < ì…ë ¥ ì œí’ˆ id * 0.01 ({threshold:.2f})")
        print(f"[INFO] 2ì°¨ ìŠ¤ìº”ì„ ì…ë ¥ ì œí’ˆ id({input_product_id})ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        print("-" * 60 + "\n")

        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,
            found_products=found_products,
            found_urls=found_urls,
        )
    else:
        print("\n[INFO] ì¶”ê°€ 2ì°¨ ìŠ¤ìº” ì¡°ê±´ ë¯¸ì¶©ì¡± (ì¶”ê°€ ìŠ¤ìº” ì—†ìŒ)")

    # Final summary
    print("\n" + "=" * 50)
    print("ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ (ì œí’ˆëª… + URL)")
    print("=" * 50)

    if not found_products:
        print("ì°¾ì€ ì œí’ˆ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for idx, (name, url) in enumerate(found_products, 1):
        print(f"{idx}. {name}")
        print(f"   {url}")

    print("\nì´ ë°œê²¬ ì œí’ˆ ìˆ˜:", len(found_products))

def scan_for_slack(product_url: str):
    """
    Slack botìš© ì—”íŠ¸ë¦¬ í•¨ìˆ˜
    - ë„ë©”ì¸ë³„ .txt íŒŒì¼ì—ì„œ ë§ˆì§€ë§‰ ID í™•ì¸
    - ìŠ¤ìº” í›„ ê²°ê³¼ë¥¼ .txt íŒŒì¼ì— ì €ì¥
    - ê¸°ì¡´ + ì‹ ê·œ ì œí’ˆ ëª¨ë‘ ë°˜í™˜
    """
    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        raise ValueError("Unsupported product URL pattern")

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        raise ValueError("Failed to extract product id from URL")

    # ë„ë©”ì¸ ì¶”ì¶œ
    domain = get_domain_from_url(product_url)
    
    # ê¸°ì¡´ ì œí’ˆ ë¡œë“œ
    existing_products = load_existing_products(domain)
    existing_urls = {url for _, url in existing_products}
    
    # ë§ˆì§€ë§‰ ID í™•ì¸
    last_id = get_last_id_from_file(domain)
    start_id = last_id + 1 if last_id > 0 else 1
    
    print(f"[INFO] ë„ë©”ì¸: {domain}")
    print(f"[INFO] ê¸°ì¡´ ì œí’ˆ ìˆ˜: {len(existing_products)}")
    print(f"[INFO] ë§ˆì§€ë§‰ ID: {last_id}")
    print(f"[INFO] ìŠ¤ìº” ì‹œì‘ ID: {start_id}")

    # ìŠ¤ìº” ì‹œì‘
    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=start_id,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=existing_urls.copy(),
    )

    # 2ì°¨ ìŠ¤ìº” ì¡°ê±´ í™•ì¸ (ì…ë ¥ IDê°€ start_idë³´ë‹¤ í¬ê³ , ë°œê²¬ ìˆ˜ê°€ ì ì„ ë•Œ)
    if input_product_id > start_id and len(found_products) < (input_product_id * 0.01):
        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,
            found_products=found_products,
            found_urls=found_urls,
        )

    # ê¸°ì¡´ + ì‹ ê·œ ì œí’ˆ í•©ì¹˜ê¸°
    all_products = existing_products + found_products
    
    # íŒŒì¼ì— ì €ì¥ (ì „ì²´ ì œí’ˆ)
    save_products_to_file(domain, all_products)
    
    # ì¸í”Œë£¨ì–¸ì„œëª… ì¶”ì¶œ
    all_product_names = [name for name, _ in all_products]
    influencers = extract_influencer_names(all_product_names)
    
    # ì¸í”Œë£¨ì–¸ì„œëª… íŒŒì¼ ì €ì¥
    influencer_file = save_influencers_to_file(domain, influencers)
    
    # ì‹ ê·œ ì œí’ˆ ìˆ˜ ì¶œë ¥
    print(f"[INFO] ì‹ ê·œ ë°œê²¬ ì œí’ˆ: {len(found_products)}ê°œ")
    print(f"[INFO] ì „ì²´ ì œí’ˆ: {len(all_products)}ê°œ")
    print(f"[INFO] ì¸í”Œë£¨ì–¸ì„œëª…: {len(influencers)}ê°œ")

    return all_products, found_products, influencer_file

if __name__ == "__main__":
    main()
