import time
import re
from urllib.parse import urlparse, urlunparse, parse_qs

import requests

# ----------------------------
# Settings
# ----------------------------
NOT_FOUND_KEYWORDS = [
    "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì¡´ì¬í•˜ì§€",
    "ì‚­ì œëœ", "íŒë§¤ì¤‘ì§€", "ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
    "ì—†ëŠ” ìƒí’ˆ", "not found", "404"
]

SLEEP_SEC = 1.0
STOP_AFTER_CONSECUTIVE_MISSES = 100
TIMEOUT_SEC = 10

USER_AGENT = "Mozilla/5.0 (compatible; ProductPageScanner/1.9)"

# ----------------------------
# URL utils
# ----------------------------
def ensure_scheme(url: str) -> str:
    url = url.strip()
    if not re.match(r"^https?://", url, re.IGNORECASE):
        return "https://" + url
    return url

def normalize_home(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, "", "", "", ""))

def strip_query_fragment(url: str) -> str:
    u = ensure_scheme(url)
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, p.path, "", "", ""))

def is_homepage(url: str) -> bool:
    p = urlparse(url)
    return (p.path or "").rstrip("/") in ["", "/"]

# ----------------------------
# Product ID extraction
# ----------------------------
def extract_product_id_from_input_url(product_url: str) -> int | None:
    """
    Extract product id from supported input URL patterns.
    - Cafe24 A: /surl/p/{id}
    - Cafe24 B: /product/.../{id}/category/...
      (id is the number right before '/category/')
    - Imweb: /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    p_clean = urlparse(clean)
    p_raw = urlparse(raw)

    path = p_clean.path or ""
    query = p_raw.query or ""

    # Cafe24 A
    m = re.search(r"/surl/p/(\d+)", path)
    if m:
        return int(m.group(1))

    # Cafe24 B (id right before /category/)
    m = re.search(r"/product/.+/(\d+)/category/", path)
    if m:
        return int(m.group(1))

    # Imweb idx
    if path.rstrip("/").lower().endswith("/product"):
        qs = parse_qs(query)
        if "idx" in qs and qs["idx"]:
            v = qs["idx"][0]
            if re.match(r"^\d+$", v):
                return int(v)

    return None

# ----------------------------
# Platform detection (ê°ì§€ìš© íŒ¨í„´ í™•ì¥) + ìŠ¤ìº” í…œí”Œë¦¿ í™•ì •
# ----------------------------
def detect_platform_from_product_url(product_url: str):
    """
    ê°ì§€ìš© íŒ¨í„´:
    - Cafe24:
        1) /surl/p/{id}
        2) /product/.../{id}/category/... (ê°ì§€ ì „ìš©)
       â†’ ê°ì§€ í›„ ìŠ¤ìº”ì€ í•­ìƒ /surl/p/{id}
    - Imweb:
        /Product/?idx={id}
    """
    raw = ensure_scheme(product_url)
    clean = strip_query_fragment(raw)

    parsed_clean = urlparse(clean)
    parsed_raw = urlparse(raw)

    path = parsed_clean.path or ""
    query = parsed_raw.query or ""
    base = normalize_home(clean)

    # ---- Cafe24 (A): /surl/p/{id}
    if "/surl/p/" in path and re.search(r"/surl/p/\d+", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    # ---- Cafe24 (B): /product/.../{id}/category/...  (ê°ì§€ ì „ìš©)
    if path.startswith("/product/") and re.search(r"/product/.+/\d+/category/", path):
        return "cafe24", f"{base}/surl/p/{{id}}"

    # ---- Imweb: /Product/?idx={id}
    if path.rstrip("/").lower().endswith("/product"):
        if re.search(r"(?:^|&)idx=\d+(?:&|$)", query, re.IGNORECASE):
            return "imweb", f"{base}/Product/?idx={{id}}"

    return None, None

# ----------------------------
# Product name parsing
# ----------------------------
def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()

def extract_product_name(html: str) -> str:
    patterns = [
        r'<meta[^>]+property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']',
        r'<meta[^>]+name=["\']twitter:title["\'][^>]*content=["\']([^"\']+)["\']',
        r"<title[^>]*>(.*?)</title>",
    ]
    for pat in patterns:
        m = re.search(pat, html, re.IGNORECASE | re.DOTALL)
        if m:
            return clean_text(m.group(1))
    return "(ì œí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨)"

# ----------------------------
# Not-found íŒë‹¨
# ----------------------------
def looks_not_found(status_code: int, requested_url: str, final_url: str, html: str) -> bool:
    if status_code != 200:
        return True

    # ì—†ëŠ” ìƒí’ˆì´ë©´ í™ˆìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ëŠ” ì¼€ì´ìŠ¤
    if requested_url.rstrip("/") != final_url.rstrip("/") and is_homepage(final_url):
        return True

    sample = (html[:20000] or "").lower()
    for kw in NOT_FOUND_KEYWORDS:
        if kw in sample:
            return True

    if len(sample.strip()) < 200:
        return True

    return False

# ----------------------------
# Scanner (1-pass)
# ----------------------------
def scan_pass(
    template_url: str,
    start_id: int,
    stop_after_consecutive_misses: int,
    sleep_sec: float,
    allow_extra_retry_if_zero_found: bool,
    found_products: list[tuple[str, str]] | None = None,
    found_urls: set[str] | None = None,
):
    """
    One scanning pass.
    - starts from start_id
    - stops when consecutive misses reach stop_after_consecutive_misses
    - optional extra retry ONLY when allow_extra_retry_if_zero_found=True
      and found_products is still empty at first stop trigger.
    """
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    product_id = start_id
    consecutive_misses = 0
    extra_retry_used = False

    if found_products is None:
        found_products = []
    if found_urls is None:
        found_urls = set()

    while True:
        url = template_url.format(id=product_id)
        print(f"[CHECK] {url}")

        try:
            r = session.get(url, allow_redirects=True, timeout=TIMEOUT_SEC)

            if looks_not_found(r.status_code, url, r.url, r.text or ""):
                consecutive_misses += 1
                print(f"  -> NOT FOUND ({consecutive_misses}/{stop_after_consecutive_misses})")
            else:
                consecutive_misses = 0
                final_url = r.url

                if final_url not in found_urls:
                    name = extract_product_name(r.text or "")
                    found_products.append((name, final_url))
                    found_urls.add(final_url)

                print(f"  âœ… FOUND: {final_url}")

        except requests.RequestException as e:
            consecutive_misses += 1
            print(f"  -> ERROR: {e} ({consecutive_misses}/{stop_after_consecutive_misses})")

        if consecutive_misses >= stop_after_consecutive_misses:
            if allow_extra_retry_if_zero_found and (len(found_products) == 0) and (not extra_retry_used):
                print(f"\n[INFO] ì•„ì§ ì œí’ˆì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í•´ ì¶”ê°€ {stop_after_consecutive_misses}íšŒ ìŠ¤ìº”ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                consecutive_misses = 0
                extra_retry_used = True
            else:
                break

        product_id += 1
        time.sleep(sleep_sec)

    return found_products, found_urls

# ----------------------------
# Main
# ----------------------------
def main():
    print("ì œí’ˆ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (UTM í¬í•¨ ê°€ëŠ¥)")
    print("ì˜ˆ) https://brainology.kr/surl/p/10")
    print("ì˜ˆ) https://brainology.kr/product/.../10/category/24/display/1/  (ê°ì§€ ì „ìš©, ìŠ¤ìº”ì€ /surl/p/{id})")
    print("ì˜ˆ) https://www.realcumin.kr/Product/?idx=72")
    product_url = input("> ").strip()

    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        print("\n[ERROR] ì²˜ìŒ ë³´ëŠ” í˜ì´ì§€ íŒ¨í„´ì…ë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        print("\n[ERROR] ì…ë ¥ URLì—ì„œ ì œí’ˆ idë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì…ë ¥í•œ ì£¼ì†Œ: {product_url}")
        return

    print(f"\n[INFO] í”Œë«í¼: {platform}")
    print(f"[INFO] ì‹¤ì œ ìŠ¤ìº” URL íŒ¨í„´: {template_url}")
    print(f"[INFO] ì…ë ¥ URL ì œí’ˆ id: {input_product_id}")
    print(f"[INFO] ì¤‘ë‹¨ ê¸°ì¤€: ì—°ì† {STOP_AFTER_CONSECUTIVE_MISSES}íšŒ NOT FOUND/ERROR")
    print(f"[INFO] ìŠ¤ìº” ì†ë„: {SLEEP_SEC}ì´ˆì— 1íšŒ")
    print("\n[START] 1ì°¨ ìŠ¤ìº” (start=1)\n")

    # 1) First pass: start at 1, with "extra retry" if zero found
    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=1,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=set(),
    )

    # 2) Conditional second pass
    threshold = input_product_id * 0.01  # as requested
    if len(found_products) < threshold:
        print("\n" + "-" * 60)
        print("[INFO] ì¶”ê°€ ì¡°ê±´ íŠ¸ë¦¬ê±°!")
        print(f"[INFO] 1ì°¨ ë°œê²¬ ê°œìˆ˜({len(found_products)}) < ì…ë ¥ ì œí’ˆ id * 0.01 ({threshold:.2f})")
        print(f"[INFO] 2ì°¨ ìŠ¤ìº”ì„ ì…ë ¥ ì œí’ˆ id({input_product_id})ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        print("-" * 60 + "\n")

        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,  # ìš”êµ¬ì‚¬í•­ëŒ€ë¡œ: ì—°ì† 100ë²ˆ ì•ˆ ë‚˜ì˜¬ ë•Œê¹Œì§€
            found_products=found_products,
            found_urls=found_urls,
        )
    else:
        print("\n[INFO] ì¶”ê°€ 2ì°¨ ìŠ¤ìº” ì¡°ê±´ ë¯¸ì¶©ì¡± (ì¶”ê°€ ìŠ¤ìº” ì—†ìŒ)")

    # Final summary
    print("\n" + "=" * 50)
    print("ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ (ì œí’ˆëª… + URL)")
    print("=" * 50)

    if not found_products:
        print("ì°¾ì€ ì œí’ˆ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for idx, (name, url) in enumerate(found_products, 1):
        print(f"{idx}. {name}")
        print(f"   {url}")

    print("\nì´ ë°œê²¬ ì œí’ˆ ìˆ˜:", len(found_products))

def scan_for_slack(product_url: str):
    """
    Slack botìš© ì—”íŠ¸ë¦¬ í•¨ìˆ˜
    - Slackì—ì„œëŠ” product_url í•˜ë‚˜ë§Œ ë„˜ê¸°ë©´ ë¨
    - ë‚´ë¶€ ë¡œì§ì€ CLIì™€ ë™ì¼
    """
    platform, template_url = detect_platform_from_product_url(product_url)
    if not platform:
        raise ValueError("Unsupported product URL pattern")

    input_product_id = extract_product_id_from_input_url(product_url)
    if input_product_id is None:
        raise ValueError("Failed to extract product id from URL")

    # 1ì°¨ ìŠ¤ìº”
    found_products, found_urls = scan_pass(
        template_url=template_url,
        start_id=1,
        stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
        sleep_sec=SLEEP_SEC,
        allow_extra_retry_if_zero_found=True,
        found_products=[],
        found_urls=set(),
    )

    # ì¡°ê±´ë¶€ 2ì°¨ ìŠ¤ìº”
    if len(found_products) < (input_product_id * 0.01):
        found_products, found_urls = scan_pass(
            template_url=template_url,
            start_id=input_product_id,
            stop_after_consecutive_misses=STOP_AFTER_CONSECUTIVE_MISSES,
            sleep_sec=SLEEP_SEC,
            allow_extra_retry_if_zero_found=False,
            found_products=found_products,
            found_urls=found_urls,
        )

    return found_products

if __name__ == "__main__":
    main()
